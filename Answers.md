# Technical Assessment: `Lead Data Manager`

## Coding task:

The project uses Python 3.8.3

The necessary requirements are included in the `requirements.txt` file.

To run use 

```
python entry.py
```

This will generate two .csv files, `scores_table.csv` and `historical_percentiles_table.csv`. 
The tables have been already generated and uploaded as `tables.zip`

The table `historical_percentiles_table` can be used to query for past percentiles for a given asset `id`, and using the `run_id` field, these percentiles can be ordered chronologically and it's possible to identify which batch was being processed when that percentile was calculated. This way, it should be possible to detect if an asset percentile decreases with time, that the incremental drift in the output of the score is becoming significant enough to affect the percentiles informed to the client.


## Conceptual assessment:

1. Since the databases are in AWS, I would deploy the ETL to Amazon EKS in a docker container using kubernetes. Whether the database credentials are managed using AWS Secrets Manager, or if a different service is being used to handle credentials, the service will need the database credentials both to read and write, since instead of reading from json files, it will query directly from the database and once the necessary transformations were made, it will wrtie the results directly to the corresponding tables in the database. I would include a crontab file as part of the project to be able to set up a schedule for the service to run, if it's needed to run once every hour, or every two hours, depending on how often new data is being generated by the model and how often the transoformed data is needed for reporting purposes.


2. If the team uses a service like Slack, it's possible to create a channel dedicated to data monitoring and alerts. The monitoring service could send alerts to slack into that specific channel to let know whenever the drift in the scores is affecting the percentiles of a certain asset as well as sending daily monitoring messages, informing for example the current size of the `scores_table`. Using the same service that the team uses to communicate regularly guarantees that the team will see the alerts, since they will have slack opened while they are working.
A task could be included in the ETL server that was described in 1. sheduled with the crontab file after the main etl task runs, that could query, for all the assets that had a new percentile calculated, which were the previous percentiles, and in the case of an asset getting a different percentile assigned than the one calculated with previous distributions of the data, send an alert to slack including the asset id, the previous percentile and the new percentile. This way, the team would get notify whenever the drift in the model results is having effects in the percentiles informed to the clients.
Another task could be added to the ETL server, and scheduled via the crontab file to run daily for example, that queries the database and obtains the size of `scores_table`. This is possible using a query with, for example, `pg_total_relation_size` that gets the size of both the table and its index and combining it with the function `pg_size_pretty`to convert it to human readable format. The size obtained with this query could be informed in a daily slack message, to monitor daily the size of the table, and as well a benchmark could be set and raise an alert whenever the table size is higher than that benchmark. 
If there is already an alert an monitoring service in place, it would possibly be better to include this tasks in that service instead of the ETL service.
At the same time, AWS allows to set up alerts regarding CPU use, memory use and tables sizes as well.